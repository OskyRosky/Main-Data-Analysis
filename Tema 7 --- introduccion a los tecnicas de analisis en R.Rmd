---
title: 'Tema 7: Introducci√≥n a los m√©todos de an√°lisis en R'

output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

suppressWarnings(library(magrittr))
suppressWarnings(library(dplyr))
suppressWarnings(library(readxl))
suppressWarnings(library(tidyr))
suppressWarnings(library(DT))

suppressWarnings(library(magrittr)) 
suppressWarnings(library(dplyr))   
suppressWarnings(library(stats))
suppressWarnings(library(FactoMineR))
suppressWarnings(library(ade4))
suppressWarnings(library(amap))
suppressWarnings(library(ggplot2))
suppressWarnings(library(factoextra))

suppressWarnings(library(FactoMineR))
suppressWarnings(library(factoextra))
#library("grid")
suppressWarnings(library(REdaS))
suppressWarnings(library(corpcor))
suppressWarnings(library(GPArotation))
suppressWarnings(library(psych))
suppressWarnings(library(ggplot2))
suppressWarnings(library(MASS))
suppressWarnings(library(GGally))
suppressWarnings(library(corrplot))
suppressWarnings(library(Hmisc))
suppressWarnings(library(psych))
suppressWarnings(library(corrplot))
suppressWarnings(library(nFactors))

suppressWarnings(library(mclust))
suppressWarnings(library(reshape2))
suppressWarnings(library(MVN))
suppressWarnings(library(stats))
suppressWarnings(library(cluster))
suppressWarnings(library(mclust))
suppressWarnings(library(dendextend))
suppressWarnings(library(igraph))
suppressWarnings(library(ape))
suppressWarnings(library(NbClust))
suppressWarnings(library(factoextra))
suppressWarnings(library(ggpubr))
suppressWarnings(library(purrr))
suppressWarnings(library(clustertend))
suppressWarnings(library(fpc))
suppressWarnings(library(pheatmap))
suppressWarnings(library(dendextend))
suppressWarnings(library(clValid))
suppressWarnings(library(klaR))
suppressWarnings(library(boot))
suppressWarnings(library(broom))
suppressWarnings(library(forecast))
suppressWarnings(library(tseries))
suppressWarnings(library(ggfortify))


personality <- read.csv("personality0.txt", sep="")
attach(personality)

personality = as.data.frame(scale(personality))

data(wine)
colnames(wine)

wine.1 <- wine[,-c(1,2)]
colnames(wine.1)


```

<style>
table {
background-color:#FFFFFF;
}
</style>

<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: darkblue;
}
</style>

<button onclick="document.body.scrollTop = document.documentElement.scrollTop = 0;" style="
    position: fixed;
    bottom: 5px;
    right: 40px;
    text-align: center;
    cursor: pointer;
    outline: none;
    color: #fff;
    background-color: #0A71A0;
    border: none;
    border-radius: 15px;
    
">Ir arriba</button>

# M√©todos de an√°lisis en R


El presente tema presenta algunos m√©todos de an√°lisis en R. 

Los temas cubiertos ser√°n:

- Muestreo.
- Componentes principales.
- An√°lisis de Factores.
- Agrupamiento (cluster).
- Discriminante. 
- Regresi√≥n.
- Regresi√≥n dicot√≥mica.
- Series temporales.

Al finalizar la sesi√≥n usted tendr√° una *noci√≥n* de una gama de an√°lisis que se pueden realizar en R, esperando que incentive a la persona a seguir buscando otras formas, m√©todos y t√©cnicas de an√°lisis de la informaci√≥n.

# 1. M√©todos de muestreo: selecci√≥n de una muestra.

## ¬øQu√© es el muestreo?

El muestreo es el proceso de seleccionar un conjunto de individuos de una poblaci√≥n con el fin de estudiarlos y poder caracterizar el total de la poblaci√≥n. Dada la imposibilidad de estudiar o an√°lisis a una poblaci√≥n entera (censar), debemos pasar el una muestreo, o por el m√©todo de muestreo. 

El muestreo se constituye de dos partes:

![](muestreo-esquema.png)

No s√© cubrir√° el tama√±o de muestre (n), y sus distintos dise√±os (aleatorio, sistem√°tico, estratificado, conglomerado o complejo, etc) pero veremos dos formas de seleccionar casos en R.

## Muestra Aleatoria

Podemos tomar una muestra de tama√±o **n** mediante la funci√≥n *"sample_n"*. Tomemos una muestra de tama√±o 50.

```{r}
dim(iris)

n <- iris %>% sample_n(50,replace=FALSE)
dim(n)
```

## Muestra seg√∫n fracci√≥n de la poblaci√≥n

Tambi√©n podemos seleccionar de forma aleatoria las observaciones pero de forma relativo o por una proporci√≥n. 

Utilizamos la funci√≥n *"sample_frac"*. Tomemos un 10% de la poblaci√≥n.

```{r}
dim(iris)

n2 <- iris %>% sample_frac(0.1,replace=FALSE)
dim(n2)
```

## Otras funciones

R posee dos librer√≠as las siguientes librer√≠as para tratar los temas de muestreo:

- **sampling**
- **survey**

Mientras que *sampling* trabaja dise√±os aleatorios irrestrictos, sistem√°ticos y jer√°rquicos, *survey* se utiliza para dise√±os complejos o por conglomerados. 

# Tipo de an√°lisis - NS.

Abordamos dos tipos de enfoque: 

- Aprendizaje no supervisado y
- Aprendizaje supervisado.

![](clasificacion-de-machine-learning.jpg)

Antes: 

- ¬øQu√© es Machine Learning?
- ¬øCu√°l es la diferencia entre el Machine Learing y los m√©todos de Estad√≠stica?

Una definici√≥n de la web explica:

*Aprendizaje no supervisado es un m√©todo donde un modelo se ajusta a las observaciones. Se distingue del Aprendizaje supervisado por el hecho de que no hay un conocimiento a priori. En el aprendizaje no supervisado, un conjunto de datos de objetos de entrada es tratado.*

En mis palabras, dir√≠a:

- En el aprendizaje no supervisado, todas las variables tienen la misma relaci√≥n entre ellas (relaci√≥n sim√©trica). Ninguna busca explicar la relaci√≥n o el comportamiento de las otras. Todas tienen el mismo nivel de explicaci√≥n.
- En el aprendizaje supervisado una variable trata de ser explicada por las dem√°s (relaci√≥n asim√©trica). Ac√° es donde se suele hablar de variable(s) dependiente(s) y variable(s) independiente(s).

Empecemos primero los an√°lisis de *aprendizaje no supervisado*.

# 2. An√°lisis por Componentes Principales.

![](pca.png)

El an√°lisis por componente principales hace parte de un grupo de an√°lisis descriptivos multidimensionales llamados m√©todos factoriales.

Son m√©todos descriptivos, no se apoyan en modelos probabil√≠sticos sino m√°s bien de un modelo geom√©trico para mejorar la representaci√≥n multidimencional.

A partir de una matriz rectangular de datos con *p* variables cuantitativas y *n* unidades, el an√°lisis por componentes principales propone diversas representaci√≥n geom√©tricas para el entendimiento de los individuos y las variables.

Lo que se busca es ver si existe una estructura, no conocida a priori, para el conjunto de casos y variables, y as√≠ mejorar la interpretaci√≥n. 

Como todo m√©todo descriptivo, llevar a cabo un PCA no es un fin en s√≠. El PCA sirve para conocer mejor los datos, detectar valores sospechosos, y ayuda a formular hip√≥tesis que se deben estudiar luego mediante modelos predictivos. 

Para la aplicaci√≥n del PCA en R, se pueden utilizar diversas librer√≠as y funciones. Se exponen algunos ejemplos:

![](pca-2.png)

Veamos la estrucuta de las salidas del an√°lisis por PCA.

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

decathlon2.active <- decathlon2[1:23, 1:10]

res.pca <- prcomp(decathlon2.active, scale = TRUE)

eig.val <- get_eigenvalue(res.pca)


```


## Variancia explicada {.tabset .tabset-fade}

### Porcentaje de la variancia explicada

Veamos el porcentaje de variancia explicada por componente

Proporci√≥ de la varianza explicada

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE, results='hide'}
prop_varianza <- res.pca$sdev^2 / sum(res.pca$sdev^2)
prop_varianza*100

ggplot(data = data.frame(prop_varianza, pc = 1:10),
       aes(x = pc, y = prop_varianza)) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")
```

Proporci√≥ de la varianza explicada acumulada


```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
prop_varianza_acum <- cumsum(prop_varianza)

ggplot(data = data.frame(prop_varianza_acum, pc = 1:10),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
```


### Gr√°fico de sedimentaci√≥n

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_eig(res.pca)
```

## Proyecciones {.tabset .tabset-fade}

El PCA busca mediante proyecci√≥n explicar a:

- Las observaciones o individuos.
- Las variables. 
- La conjunci√≥n de observaciones-variables.

### Individuos

Proyecci√≥n de los individuos

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
data(decathlon2)
decathlon2.active <- decathlon2[1:23, 1:10]

res.pca <- prcomp(decathlon2.active, scale = TRUE)

fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```

### Variables

Proyecci√≥n de las variables

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```

### Individuos - Variables

Proyecci√≥n de individuos y variables

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```


# 3. An√°lisis de factores.

En el AF la atenci√≥n se centra principalmente en las variables estad√≠sticas y no en los individuos; es m√°s un m√©todo de *an√°lisis multivariante* que un m√©todo de *an√°lisis multidimensional*. 

El An√°lisis Factorial es un nombre gen√©rico que se da a una clase de m√©todos estad√≠sticos multivariantes cuyo prop√≥sito principal es definir la estructura subyacente en una matriz de datos. Generalmente hablando, aborda el problema de c√≥mo analizar la estructura de las interrelaciones (correlaciones) entre un gran n√∫mero de variables.

Al igual que el PCA, en el AF se busca reducir un n√∫mero *p* a *k* variables. Sin embargo, el AF posee ciertas caracter√≠sticas que lo hacen el an√°lisis ideal para conocer las estructura subyacente que se desea estudiar.  Se dice que el AF es m√°s elaborado, y se buscan conclusiones m√°s contundentes, el PCA busca m√°s aproximaciones a nivel gr√°fico del conjunto de datos. A esa reducci√≥n les llamamos Factores. 

De forma matem√°tica, el **PCA** es un **AF** pero m√°s simple: sin extracci√≥n de vector propio, sin rotaciones, y sin transformaciones, adem√°s del gran supuesto practicamente irreal de la ortogonalidad de las variables o componentes principales en el PCA.

Desde un punto de vista pr√°ctico, el PCA es observacional, y el AF es un enfoque para un modelo explicito (estructura subyacente).

Mientras que el PCA busca el an√°lisis de la variancia, el AF adem√°s busca el an√°lisis de la covariancia y correlaci√≥n entre las variables.

El objetivo t√≠pico en el an√°lisis factorial es identificar las variables que est√°n relacionadas entre s√≠, y separarlas de otras (una forma de agrupaci√≥n de las variable). Los tipos de rotaciones ayudan a visualizar mejor este proceso. 

Existen dos grandes tipos de modalidaes para el FA: 

- An√°lisis Factorial Exploratorio.
- An√°lisis Factorial Confirmatorio.


```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
wine.1 <- wine[,-c(1,2)]

```


```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

personality <- read.csv("personality0.txt", sep="")

attach(personality)

personality = as.data.frame(scale(personality))

```


```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

 res.mfa <- MFA(wine, 
               group = c(2, 5, 3, 10, 9, 2), 
               type = c("n", "s", "s", "s", "s", "s"),
               name.group = c("origin","odor","visual",
                              "odor.after.shaking", "taste","overall"),
               num.group.sup = c(1, 6),
               graph = FALSE)

```


```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fa.0 <- fa(r=wine.1, nfactors = 10, rotate = "none")
```


```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
res1b <- factanal(personality, factors = 10, rotation = "none", na.action = na.omit)
#Las diversas funciones del an√°lisis de factores:
```

Veamos unos usos del FA exploratorio.

## Cantidad de Factores {.tabset .tabset-fade}

### Gr√°fico de sedimentaci√≥n (Scree plot)

#### Opci√≥n 1

La forma m√°s popular de conocer la cantidad √≥ptima de factores es mediante el gr√°fico de sedimentaci√≥n. Lo usual es ver d√≥nde es que se lleva a cabo la ca√≠da m√°s
importante, o se dibuja un "codo". A veces no es nada f√°cil el saber lo anterior. Personalmente considero que ante la poca claridad, hay que llevar a cabo un an√°lisis
interactivo del FA.

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_screeplot(res.mfa)
```

¬øCu√°ntos factores?

#### Opci√≥n 2

R posee ciertos criterios que permiten "guiar" al analista en la elecci√≥n del n√∫mero de factor a elegir.

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

corpdat1 <- cor(wine.1, use="pairwise.complete.obs")

fa.parallel(x=corpdat1, fm="minres", fa="fa")

nScree(x=corpdat1,model="factors")
plot(nScree(x=corpdat1,model="factors"))
```

Entonces, ¬øcu√°ntos componentes debemos elegir? 



```{r error=FALSE ,message=FALSE, warning=FALSE, echo=TRUE}
fa.1 <- fa(r=wine.1, nfactors = 3, rotate = "varimax")

res1a <- factanal(personality, factors = 5, rotation = "varimax", na.action = na.omit)
fa.0 <- fa(r=personality, nfactors = 5, rotate = "varimax")
res1a <- factanal(personality, factors = 5, rotation = "varimax", na.action = na.omit)
res1a <- factanal(personality, factors = 5, rotation = "varimax", na.action = na.omit)
res1a <- factanal(personality, factors = 5, rotation = "varimax", na.action = na.omit)
```


```{r error=FALSE ,message=FALSE, warning=FALSE} 

# Calculadon la  singularidad  y la comunalidad

loadings_distant = res1a$loadings[1,]
communality_distant = sum(loadings_distant^2); communality_distant


uniqueness_distant = 1-communality_distant; uniqueness_distant
```


## Verificaci√≥n de resultados {.tabset .tabset-fade}

### Contribuciones

Veamos el peso de cada variable a los Factores

Factor 1

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_contrib(res.mfa, choice = "quanti.var", axes = 1, top = 20,
             palette = "jco")
```

Factor 2

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_contrib(res.mfa, choice = "quanti.var", axes = 2, top = 20,
             palette = "jco")
```

Factor 3

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fviz_contrib(res.mfa, choice = "quanti.var", axes = 3, top = 20,
             palette = "jco")
```

### Reporte del FA

Seg√∫n la cantidad de factores, la funci√≥n fa.diagram nos puede indicar cu√°l es la estructura factorial del an√°lisis.

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
fa.1 <- fa(r=wine.1, nfactors = 3, rotate = "varimax")

fa.diagram(fa.1)
```


### Factores en plano 2D

LLevado a cabo el proceso de elegir la rotaci√≥n y luego el n√∫mero de componentes, podemos verificar que desde un inicio hasta la soluci√≥n del problema, la estructura factorial 
del FA posee una forma de explicar los facores en un plano 2D. Veamos esto

Inicio: todas las variables

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
res1b <- factanal(personality, factors = 10, rotation = "none", na.action = na.omit)

load = res1b$loadings[,1:2]
plot(load, type="n") 
text(load,labels=names(personality),cex=.7)
```

Intermedio: aplicaci√≥n del FA y el acomodo de las variables

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

res1a <- factanal(personality, factors = 5, rotation = "varimax", na.action = na.omit)

load <- res1a$loadings[,1:2]
plot(load, type="n") # set up plot 
text(load,labels=names(personality),cex=.7) # agregando el nombre de las variables

```

Final: estructura final del FA

Creamos la estructura factorial mediante el an√°lisis de las cargas factoriales

```{r }
shy = rowMeans(cbind(personality$distant, personality$shy, personality$withdrw, personality$quiet))
outgoing = rowMeans(cbind(personality$talkatv, personality$outgoin, personality$sociabl))
hardworking = rowMeans(cbind(personality$hardwrk, personality$persevr, personality$discipl))
friendly = rowMeans(cbind(personality$friendl, personality$kind, personality$coopera, personality$agreebl, personality$approvn, personality$sociabl))
anxious = rowMeans(cbind(personality$tense, personality$anxious, personality$worryin))

# Combinando los factores y creando una nueva estructura de datos
combined_data = cbind(shy,outgoing,hardworking,friendly,anxious)
combined_data = as.data.frame(combined_data)
```

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

res2 <- factanal(combined_data, factors = 2, na.action=na.omit)

load <- res2$loadings[,1:2]
plot(load, type="n") # set up plot 
text(load,labels=names(combined_data),cex=.7) # agregando el nombre de las variables

```

Puede que ac√° se estimen diversos modelos de FA, con diversas rotaciones, con tal de ver una mejorar separaci√≥n, y hasta aglomeraci√≥n de los factores. 


# 4. An√°lisis de correspondencia.

Es una t√©cnica descriptiva o exploratoria cuyo objetivo es resumir una gran cantidad de datos en un n√∫mero reducido de dimensiones, con la menor p√©rdida de informaci√≥n posible. En esta l√≠nea, su objetivo es similar al de los m√©todos factoriales, salvo que en el caso del an√°lisis de correspondencias el m√©todo se aplica sobre variables categ√≥ricas (nominales y ordinales).

El an√°lisis de correspondencias simples se utiliza a menudo en la representaci√≥n de datos que se pueden presentar en forma de tablas de contingencia de dos variables nominales u ordinales. Otras utilizaciones implican el tratamiento de tablas de proximidad o distancia entre elementos, y tablas de preferencias.

El an√°lisis de correspondencias consiste en resumir la informaci√≥n presente en las filas y columnas de manera que pueda proyectarse sobre un subespacio reducido, y representarse simult√°neamente los puntos fila y los puntos columna, pudi√©ndose obtener conclusiones sobre relaciones entre las dos variables nominales u ordinales de origen.

La extensi√≥n del an√°lisis de correspondencias simples al caso de varias variables nominales (tablas de contingencia multidimensionales) se denomina An√°lisis de Correspondencias M√∫ltiples, y utiliza los mismos principios generales que la t√©cnica anterior. En general se orienta a casos en los cuales una variable representa √≠tems o individuos y el resto son variables cualitativas u ordinales que representan cualidades.

## Librer√≠as

## Ejemplo de un CA simple

```{r}

housetasks
```

```{r}
res.ca <- CA(housetasks, graph = FALSE)
get_ca_row(res.ca)
get_ca_col(res.ca)
```

```{r}
#--- para las filas:
fviz_contrib(res.ca, choice = "row", axes = 1)
```
```{r}
#--- para las columnas:
fviz_contrib(res.ca, choice = "col", axes = 1)
```

```{r}
#--- para las variables en las filas:
fviz_ca_row(res.ca, repel = TRUE)
```

```{r}
#--- para las variables en las columnas:
fviz_ca_col(res.ca)
```

```{r}
#--- y la visualizaci√≥n conjunta:
fviz_ca_biplot(res.ca, repel = TRUE)
```


# 5. An√°lisis de agrupaci√≥n o Cluster Analysis. 

El An√°lisis por Conglomerados (Cluster Analysis), es una t√©cnica estad√≠stica multivariante que agrupa elementos (sujetos) tratando de lograr la m√°xima homogeneidad en cada grupo y la mayor diferencia entre los grupos.

Es un m√©todo basado en criterios geom√©tricos y se utiliza fundamentalmente como una t√©cnica exploratoria, descriptiva pero no explicativa. Se fundamenta en teor√≠as de optimizaciones num√©ricas, y no en optimizaciones algebraicas. 

Las soluciones no son √∫nicas, en la medida en que la pertenencia al conglomerado para cualquier n√∫mero de soluciones depende de muchos elementos del procedimiento elegido.

La soluci√≥n del Cluster depende totalmente de las variables utilizadas, la adici√≥n o destrucci√≥n de variables relevantes puede tener un impacto substancial sobre la soluci√≥n resultante.

Los algoritmos de formaci√≥n de conglomerados se agrupan en diversas categor√≠as, y existen muchas modalidades. El presente cap√≠tulo estudia los modalidades de clusters jer√°rquicos y los de k-medias.

El objetivo del an√°lisis de cluster es, a partir de *‚Äúùëõ‚Äù* individuos (ùëõ grupos  iniciales) buscar agrupar el conjunto de individios en dos o m√°s grupos basados en la similitud de esos objetos seg√∫n una serie especificada de caracter√≠sticas (variables).

Finalmente, siempre hay que tomar en cuenta los mismos tres aspectos que podr√≠an influir fuertemente en el an√°lisis de los datos:

- Valores extremos
- Medici√≥n de la similitud
- Estandarizaci√≥n de los datos

Veremos algunos ejemplos de los algoritmos presentados. 

## Funciones y librer√≠as del cluster analysis

En el entorno de programaci√≥n R existen m√∫ltiples paquetes que implementan algoritmos de clustering y funciones para visualizar sus resultados. En este documento se emplean los siguientes:

stats: contiene las funciones:

- dist() para calcular matrices de distancias
- kmeans() 
- hclust()  
- cuttree() para crear los clusters y
- plot.hclust() para visualizar los resultados.
- luster 
- mclust: contienen m√∫ltiples algoritmos de clustering y m√©tricas para evaluarlos.
- factoextra: extensi√≥n basada en ggplot2 para crear visualizaciones de los resultados de clustering y su evaluaci√≥n.
- dendextend: extensi√≥n para la customizaci√≥n de dendrogramas. 

Para evitar problemas de la magnitud de las variables, recordar **SIEMPRE** escalar las variables...

Veamos la estructura de algunas de funciones anteriores:

### scale

La estructura es :

```{r}
#scale(x, center = TRUE, scale = TRUE)
```


### kmeans

La estructura es :

```{r}
#kMeans(x, centers, iter.max=10, num.seeds=10)
```

### fviz_cluster

La estructura es :

```{r}
# fviz_cluster(object, data = NULL, choose.vars = NULL, stand = TRUE,
#  axes = c(1, 2), geom = c("point", "text"), repel = FALSE,
#  show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex",
#  ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL,
#  pointsize = 1.5, labelsize = 12, main = "Cluster plot", xlab = NULL,
#  ylab = NULL, outlier.color = "black", outlier.shape = 19,
#  ggtheme = theme_grey(), ...)
```


### hclust

La estructura es :

```{r}
# hclust(d, method = "complete", members = NULL)
# S3 method for hclust
# plot(x, labels = NULL, hang = 0.1, check = TRUE,
#      axes = TRUE, frame.plot = FALSE, ann = TRUE,
#      main = "Cluster Dendrogram",
#      sub = NULL, xlab = NULL, ylab = "Height", ‚Ä¶)
```

## Cluster por k medias

El set de datos USArrests contiene informaci√≥n sobre el n√∫mero de delitos (asaltos, asesinatos y secuestros) junto con el porcentaje de poblaci√≥n urbana para cada uno de los 50 estados de USA. Se pretende estudiar si existe una agrupaci√≥n subyacente de los estados empleando K-means-clustering.

El paquete factoextra creado contiene funciones que facilitan en gran medida la visualizaci√≥n y evaluaci√≥n de los resultados de clustering. Si se emplea K-means-clustering con distancia eucl√≠dea hay que asegurarse de que las variables empleadas son de tipo continuo, ya que trabaja con la media de cada una de ellas.

Veamos una representaci√≥n de los datos, para 4 clusters (K=4)

```{r echo=TRUE,error=FALSE ,message=FALSE, warning=FALSE}
datos <- scale(USArrests)

set.seed(123)
km_clusters <- kmeans(x = datos, centers = 4, nstart = 50)

# Las funciones del paquete factoextra emplean el nombre de las filas del
# dataframe que contiene los datos como identificador de las observaciones.
# Esto permite a√±adir labels a los gr√°ficos.
fviz_cluster(object = km_clusters, data = datos, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  labs(title = "Resultados clustering K-means") +
  theme_bw() +
  theme(legend.position = "none")
```

## Clusters jer√°rquicos

Hierarchical clustering es una alternativa a los m√©todos de partitioning clustering que no requiere que se pre-especifique el n√∫mero de clusters. Los m√©todos que engloba el hierarchical clustering se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos:

Agglomerative clustering (bottom-up): el agrupamiento se inicia en la base del √°rbol, donde cada observaci√≥n forma un cluster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en una √∫nica ‚Äúrama‚Äù central.

Divisive clustering (top-down): es la estrategia opuesta al agglomerative clustering, se inicia con todas las observaciones contenidas en un mismo cluster y se suceden divisiones hasta que cada observaci√≥n forma un cluster individual.

En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de √°rbol llamada dendrograma. 

En este se debe antes transformar los datos a distancias

```{r error=FALSE ,message=FALSE, warning=FALSE}


# Crear las distancia y definir el m√©todo 
dd <- dist(scale(USArrests), method = "euclidean")
hc <- hclust(dd, method = "ward.D2")
```

Veamos diversas modalidaes 

### Tipos de clusters jer√°rquicos

Referencia: http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning 

Mediante el plot.hclust(), el m√°s sencillo 

```{r}
plot(hc)
# Put the labels at the same height: hang = -1
plot(hc, hang = -1, cex = 0.6)

```

Mediante el plot.dendogram(),

```{r}
# Convert hclust into a dendrogram and plot
hcd <- as.dendrogram(hc)
# Default plot
plot(hcd, type = "rectangle", ylab = "Height")
# Zoom in to the first dendrogram
plot(hcd, xlim = c(1, 20), ylim = c(1,8))
# Triangle plot
plot(hcd, type = "triangle", ylab = "Height")


```

Mediante el Phylogenetic trees

Este presenta diversas formas de representar los clusters

```{r}
# Default plot
plot(as.phylo(hc), cex = 0.6, label.offset = 0.5)
# Cladogram
plot(as.phylo(hc), type = "cladogram", cex = 0.6, 
     label.offset = 0.5)
# Unrooted
plot(as.phylo(hc), type = "unrooted", cex = 0.6,
     no.margin = TRUE)
# Fan
plot(as.phylo(hc), type = "fan")
# Radial
plot(as.phylo(hc), type = "radial")

# Change the appearance
# change edge and label (tip)
plot(as.phylo(hc), type = "cladogram", cex = 0.6,
     edge.color = "steelblue", edge.width = 2, edge.lty = 2,
     tip.color = "steelblue")
```


# Tipo de an√°lisis - S.

Ahora pasemos a los an√°lisis supervisados.

En el aprendizaje supervisado una variable trata de ser explicada por las dem√°s (relaci√≥n asim√©trica, variables dependientes e independientes).

![](clasificacion-de-machine-learning.jpg)

# Clasificaci√≥n  / an√°lisis discriminante.    

La clasificaci√≥n se centra en identificar a cu√°l de un conjunto de categor√≠as (subpoblaciones) pertenece una nueva observaci√≥n, sobre la base de un conjunto de datos de formaci√≥n que contiene observaciones (o instancias) cuya categor√≠a de miembros es conocida.

En el an√°lisis de aprendizaje autom√°tico, para datos transversales, existen dos grandes categorias: la clasificaci√≥n y la predicci√≥n.

![](clasificacion-prediccion.png)

Por ahora veremos la clasificaci√≥n, luego en la regresi√≥n veremos la predicci√≥n, y volveremos a ver la clasifiaci√≥n cuando vemos la regresi√≥n dicot√≥mica. 

# 6. An√°lisis discriminante

El an√°lisis discriminante es una t√©cnica multivariante cuya finalidad es describir las diferencias significativas entre *ùëò* grupos de objetos (ùëò > 1) sobre los que se observan (variable de clasificaci√≥n).

En caso de que estas diferencias existan, intentar√° explicar en qu√© sentido se dan y proporcionar procedimientos de asignaci√≥n sistem√°tica de nuevas observaciones con grupo desconocido a uno de los grupos analizados, utilizando para ello sus valores en lasùëòvariables clasificadoras.

Podemos ver este procedimiento como un modelo de predicci√≥n de una variable respuesta categ√≥rica (variable grupo) a partir de *ùëò* variables explicativas generalmente continuas (variables clasificatorias).

El An√°lisis Discriminante consiste en buscar nuevas variables (variables discriminantes), que separan lo mejor posible los grupos de proyecci√≥n en las ùëò variables, de las ùëõ observaciones, para poder predecir nuevas observaciones *i*.

En la clasificaci√≥n se llevan a cado dos procesos fundamentales: construir la regla o ecuaci√≥n de  clasificaci√≥n, y verificar la calidad del proceso, para luego introducir nuevos casos.. 

![](etapas-clasificacion.png)

En la construcci√≥n de la regla de decisi√≥n, veremos un forma lineal y otro cuadr√°tica.

## Librer√≠as y funciones

Utilizaremos dos funciones lda() y qda() de la librer√≠a MASS.

### lda() (MASS)

La estructura es :

```{r eval=FALSE}
 lda(x, ‚Ä¶)
  S3 method for formula
 lda(formula, data, ‚Ä¶, subset, na.action)

 S3 method for default
 lda(x, grouping, prior = proportions, tol = 1.0e-4,
    method, CV = FALSE, nu, ‚Ä¶)

 S3 method for data.frame
 lda(x, ‚Ä¶)

  S3 method for matrix
  lda(x, grouping, ‚Ä¶, subset, na.action)
```


Ver el enlace: https://www.rdocumentation.org/packages/MASS/versions/7.3-51.1/topics/lda


### qda()

La estructura es :

```{r eval=FALSE}
qda(x, ‚Ä¶)
S3 method for formula
qda(formula, data, ‚Ä¶, subset, na.action)

S3 method for default
qda(x, grouping, prior = proportions,
   method, CV = FALSE, nu, ‚Ä¶)

S3 method for data.frame
qda(x, ‚Ä¶)

S3 method for matrix
qda(x, grouping, ‚Ä¶, subset, na.action)
```

Ver el enlace: https://www.rdocumentation.org/packages/MASS/versions/7.3-51.1/topics/qda

## Partici√≥n del archivo de datos

Pera evaluar la regla de decisi√≥n, se suele particionar el archivo de datos en entramiento y validaci√≥n. Esto es un procedimiento t√≠pico de los an√°lisis supervisados, y tambi√©n en la miner√≠a de datos. 

```{r  error=FALSE ,message=FALSE, warning=FALSE}
training_sample <- sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.6,0.4))

train <- iris[training_sample, ]
test <- iris[!training_sample, ]

```

Dimensiones del set de entrenamiento

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
dim(train)
```

Dimensiones del set de validaci√≥n

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
dim(test)
```


## Estimaci√≥n del lda y qda


### Estimaci√≥n del lda 

```{r}
lda.iris <- lda(Species ~ ., train)
lda.iris
names(lda.iris)
```

Enfoquemos en la salida, la parte de los coeficientes (podemos verlo vediante el "scaling")

¬øCu√°l ser√≠a entonces las ecuaciones de ambas reglas?

LD1 ?
LD2 ?

### Estimaci√≥n del qda

```{r}
qda.iris <- qda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, train)
qda.iris
```

Analisando la funci√≥n qda, no se program√≥ la ecuaci√≥n del qda...

## An√°lisis de Regla de discriminaci√≥n

Una forma de ver la efectividad de las reglas de separarci√≥n en poner en un plano 2D los casos con sus respectivas reglas si hubiese 2. 

```{r}
plot(lda.iris, col = as.integer(train$Species))
```

Tambi√©n se puede corroborar mediante el an√°lisis de la posici√≥n y variabilidad por categor√≠a de clasificaci√≥n, en cada regla. El caso de la RD1:

```{r}
lda.iris <- lda(Species ~ ., train)
lda.iris

plot(lda.iris, dimen = 1, type = "b")
```
Tengo un error para la 2nd dimensi√≥n... creo que debe ser porque explica muy poco.

```{r}
#plot(lda.iris, dimen = 2, type = "both")
```

Otra forma de ver la efectividad de las reglas es mediante la proyecci√≥n de la regla y sus 
inviduos para ciertas variables.
 
Veamos que tanto separa o discrimina la primera regla

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

X <- scale(as.matrix(iris[,-5])) # better scale mean and variance before LDA
Y <- unclass(iris$Species)
iris1 <- data.frame(X=X, Y=Y)
colnames(iris1) <- colnames(iris)
#head(iris1)

model <- lda(Species ~ . , data=iris1, prior=c(1,1,1)/3)

vec <- c(model$scaling[1,1], model$scaling[3,1])
v   <- vec / sqrt(sum(vec^2))  # make it a unit vector
lda1.points <- as.matrix(iris1[,c(1,3)]) %*% v %*% t(v) # to project point X into unit vector v just calculate X.v.v^T
plot(iris1[,"Sepal.Length"], iris1[,"Petal.Length"], 
     col=c("blue","green","red")[iris1$Species], pch=19,
     xlab="Sepal Length", ylab="Petal.Length",   main="1st discriminant functions")
segments(-vec[1],-vec[2],vec[1],vec[2])

# points(lda1.points , col=c("blue","green","red")[iris1$Species], pch=18) # draw projection point
for(i in 1:nrow(iris1)) {
  segments(iris1[i,1], iris1[i,3], lda1.points[i,1], lda1.points[i,2], 
           lty=2, col=c("blue","green","red")[iris1[i,]$Species])
}

```

Y para la 2nda regla:

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}

vec <- c(model$scaling[1,2], model$scaling[3,2])
v   <- vec / sqrt(sum(vec^2))
lda2.points <- as.matrix(iris1[,c(1,3)]) %*% v %*% t(v)
plot(iris1[,"Sepal.Length"], iris1[,"Petal.Length"], 
     col=c("blue","green","red")[iris1$Species], pch=19,
     xlab="Sepal Length", ylab="Petal.Length",   main="2nd discriminant functions")
segments(-2*vec[1],-2*vec[2],2*vec[1],2*vec[2])

# points(lda2.points , col=c("blue","green","red")[iris1$Species], pch=18) # draw projection point
for(i in 1:nrow(iris1)) {
  segments(iris1[i,1], iris1[i,3], lda2.points[i,1], lda2.points[i,2], 
           lty=2, col=c("blue","green","red")[iris1[i,]$Species])
}

```

¬øQu√© podemos concluir?

## Los gr√°ficos de partici√≥n y las proyecciones

### Particiones cl√°siscas con partimat

El uso de la funci√≥n partimat del paquete klaR proporciona una forma alternativa de trazar las funciones discriminantes lineales. partimat emite una serie de gr√°ficos para cada combinaci√≥n de dos variables. Piense en cada gr√°fico como una vista diferente de los mismos datos. Las regiones coloreadas delinean cada √°rea de clasificaci√≥n. Se predice que cualquier observaci√≥n que caiga dentro de una regi√≥n ser√° de una clase espec√≠fica. Cada gr√°fico tambi√©n incluye la tasa de error aparente para esa vista de los datos.

Partici√≥n lineal

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="lda")
```

Partici√≥n cuadr√°tica

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="qda")
```

### Particiones  partimat cambiando el color

La funci√≥n partimat() del paquete klar permite representar los l√≠mites de clasificaci√≥n de un modelo discriminante lineal o cuadr√°tico para cada par de predictores. Cada color representa una regi√≥n de clasificaci√≥n acorde al modelo, se muestra el centroide de cada regi√≥n y el valor real de las observaciones.

Partici√≥n lineal

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
partimat(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
         data = train, method = "lda", prec = 200,
         image.colors = c("darkgoldenrod1", "snow2", "skyblue2"),
         col.mean = "firebrick")
```

Partici√≥n cuadr√°tica

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
partimat(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width,
         data = train, method = "qda", prec = 200,
         image.colors = c("darkgoldenrod1", "snow2", "skyblue2"),
         col.mean = "firebrick")
```

### Proyecci√≥n del modelo: variables y predicciones

Podemos verificar la calidad del an√°lisis mediante las proyeccioes de las variables y las predicciones.

Variables: podemos ver los resultados del modelo de la separaci√≥n de los casos en los variables.

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
X <- scale(as.matrix(iris[,-5])) # better scale mean and variance before LDA
Y <- unclass(iris$Species)
iris1 <- data.frame(X=X, Y=Y)
colnames(iris1) <- colnames(iris)
model <- lda(Species ~ . , data=iris1, prior=c(1,1,1)/3)
plot(iris1[,"Sepal.Length"], iris1[,"Petal.Length"], 
     col=c("blue","green","red")[iris1$Species], pch=19,
     xlab="Sepal Length", ylab="Petal.Length")
means <- model$means
points(means[,c(1,3)], pch=3, lwd=2, col="purple")
```

¬øQu√© podemos decir?

Predicciones.

Podemos para los valores de las reglas verificar si los individuos se est√°n o no separando o aglomerando
en cierto parte.

LD1: ¬øqu√© podemos decir?

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
pred <- predict(model, iris[,-5])
# The next horizontal axis are meaningless, they depends on the sample order of the dataset

plot(pred$x[,1], col=c("blue","green","red")[iris1$Species], pch=19) # we can plot them

```

LD2: ¬øqu√© podemos decir?

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
# Notice that the 2nd discriminant function does not separate that well the 2nd & 3rd class
plot(pred$x[,2], col=c("blue","green","red")[iris1$Species], pch=19) # we can plot them
```

## Las tablas de confusi√≥n

### Set de entrenamiento

#### LDA

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
lda.train <- predict(lda.iris)
train$lda <- lda.train$class
table(train$lda,train$Species)
```

--> Recomendable tenerlo mejor en %

¬øQu√© podemos concluir del modelo para el entrenamiento?

#### QDA

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
qda.train <- predict(qda.iris)
train$qda <- qda.train$class
table(train$qda,train$Species)
```


--> Recomendable tenerlo mejor en %

¬øQu√© podemos concluir del modelo para el entrenamiento? Y comparando LDA y QDA

### Set de validaci√≥n

#### LDA

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
lda.test <- predict(lda.iris,test)
test$lda <- lda.test$class
table(test$lda,test$Species)
```


--> Recomendable tenerlo mejor en %

¬øQu√© podemos concluir del modelo para el entrenamiento?

#### QDA

```{r echo=FALSE, error=FALSE ,message=FALSE, warning=FALSE}
qda.test <- predict(qda.iris,test)
test$qda <- qda.test$class
table(test$qda,test$Species)
```

--> Recomendable tenerlo mejor en %

¬øQu√© podemos concluir del modelo para el entrenamiento? Y comparando LDA y QDA


# 7. Regresi√≥n.

El objetivo es mostrar los principales comandos en R para generar:

- Regresi√≥ lineal simple
- Regresi√≥n lineal m√∫ltiple.

De igual forma se ver√°n ciertas etapas de an√°lisis que componen a estas variantes de la regresi√≥n.

## Datos

El dataset Boston del paquete MASS recoge la mediana del valor de la vivienda en 506 √°reas residenciales de Boston. Junto con el precio, se han registrado 13 variables adicionales.

```{r}
tail(Boston)
```

- crim: ratio de criminalidad per c√°pita de cada ciudad.
- zn: Proporci√≥n de zonas residenciales con edificaciones de m√°s de 25.000 pies cuadrados.
- indus: proporci√≥n de zona industrializada.
- chas: Si hay r√≠o en la ciudad (= 1 si hay r√≠o; 0 no hay).
- nox: Concentraci√≥n de √≥xidos de nitr√≥geno (partes per 10 mill√≥n).
- rm: promedio de habitaciones por vivienda.
- age: Proporci√≥n de viviendas ocupadas por el propietario construidas antes de 1940.
- dis: Media ponderada de la distancias a cinco centros de empleo de Boston.
- rad: √çndice de accesibilidad a las autopistas radiales.
- tax: Tasa de impuesto a la propiedad en unidades de $10,000.
- ptratio: ratio de alumnos/profesor por ciudad.
- black: 1000(Bk - 0.63)^2 donde Bk es la proporci√≥n de gente de color por ciudad.
- lstat: porcentaje de poblaci√≥n en condici√≥n de pobreza.
- medv: Valor mediano de las casas ocupadas por el due√±o en unidades de $1000s.

## Regresi√≥n lineal simple

## An√°lisis descriptivos de los datos

```{r}
summary(Boston)
```

Veamos las distribuciones por variable:

```{r}
multi.hist(x = Boston[,1:3], dcol = c("blue","red"), dlty = c("dotted", "solid"),
           main = "")

multi.hist(x = Boston[,5:9], dcol = c("blue","red"), dlty = c("dotted", "solid"),
           main = "")

multi.hist(x = Boston[,10:14], dcol = c("blue","red"),
           dlty = c("dotted", "solid"), main = "")
```

vemos que los datos no poseen total normalidad en las variables.

## Regresi√≥n lineal simple

Se pretende predecir el valor de la vivienda en funci√≥n del porcentaje de pobreza de la poblaci√≥n. Empleando la funci√≥n *lm()* se genera un modelo de regresi√≥n lineal por m√≠nimos cuadrados en el que la variable respuesta es **medv** y el predictor **lstat**.

```{r}
modelo_simple <- lm(data = Boston,formula = medv ~ lstat)
modelo_simple
names(modelo_simple)
summary(modelo_simple)
```

En la informaci√≥n devuelta por el summary se observa que el p-value del estad√≠stico F es muy peque√±o, indicando que al menos uno de los predictores del modelo est√° significativamente relacionado con la variable respuesta. ¬øC√≥mo ser√≠a en palabras del presente ejemplo?


La creaci√≥n de un modelo de regresi√≥n lineal simple suele acompa√±arse de una representaci√≥n gr√°fica superponiendo las observaciones con el modelo. Adem√°s de ayudar a la interpretaci√≥n, es el primer paso para identificar posibles violaciones de las condiciones de la regresi√≥n lineal.

```{r}
attach(Boston)
plot(x = lstat, y = medv, main = "medv vs lstat", pch = 20, col = "grey30")
abline(modelo_simple, lwd = 3, col = "red")
```

La representaci√≥n gr√°fica de las observaciones muestra que la relaci√≥n entre ambas variables estudiadas no es del todo lineal, lo que apunta a que otro tipo de modelo podr√≠a explicar mejor la relaci√≥n. Aun as√≠ la aproximaci√≥n no es mala.

Una vez generado el modelo, es posible predecir el valor de la vivienda sabiendo el estatus de la poblaci√≥n en la que se encuentra. Toda predicci√≥n tiene asociado un error y por lo tanto un intervalo. Es importante diferenciar entre dos tipos de intervalo:

Intervalo de confianza: Devuelve un intervalo para el valor promedio de todas las viviendas que se encuentren en una poblaci√≥n con un determinado porcentaje de pobreza, sup√≥ngase lstat=10.

```{r}
predict(object = modelo_simple, newdata = data.frame(lstat = c(10)),
        interval = "confidence", level = 0.95)
```

Intervalo de predicci√≥n: Devuelve un intervalo para el valor esperado de una vivienda en particular que se encuentre en una poblaci√≥n con un determinado porcentaje de pobreza.


```{r}
predict(object = modelo_simple, newdata = data.frame(lstat = c(10)),
        interval = "prediction", level = 0.95)
```


Una de las mejores formas de confirmar que las condiciones necesarias para un modelo de regresi√≥n lineal simple por m√≠nimos cuadrados se cumplen es mediante el estudio de los residuos del modelo.

En R, los residuos se almacenan dentro del modelo bajo el nombre de residuals. R genera autom√°ticamente los gr√°ficos m√°s t√≠picos para la evaluaci√≥n de los residuos de un modelo.

```{r}
par(mfrow = c(1,2))
plot(modelo_simple)
par(mfrow = c(1,1))
```

Otra forma de identificar las observaciones que puedan ser outliers o puntos con alta influencia (leverage) es emplear las funciones *rstudent()* y *hatvalues()*.

```{r}
plot(x = modelo_simple$fitted.values, y = abs(rstudent(modelo_simple)),
     main = "Absolute studentized residuals vs predicted values", pch = 20,
     col = "grey30")
abline(h = 3, col = "red")

plot(hatvalues(modelo_simple), main = "Medici√≥n de leverage", pch = 20)
# Se a√±ade una l√≠nea en el threshold de influencia acorde a la regla
# 2.5x((p+1)/n)
abline(h = 2.5*((dim(modelo_simple$model)[2]-1 + 1)/dim(modelo_simple$model)[1]),
       col = "red")
```

En este caso muchos de los valores parecen posibles outliers o puntos con alta influencia porque los datos realmente no se distribuyen de forma lineal en los extremos. Deber√≠amos evaluar otros modelos y otras variables.

## Regresi√≥n m√∫ltiple

Se desea generar un modelo que permita explicar el precio de la vivienda de una poblaci√≥n empleando para ello cualquiera de las variables disponibles en el dataset Boston y que resulten √∫tiles en el modelo.

R permite crear un modelo con todas las variables incluidas en un data.frame de la siguiente forma:

```{r}
modelo_multiple <- lm(formula = medv ~ ., data = Boston)
# Tambi√©n se pueden especificar una a una 
summary(modelo_multiple)
```

En el summary se puede observar que algunos predictores tienen p-values muy altos, sugiriendo que no contribuyen al modelo por lo que deben ser excluidos, por ejemplo age e indus. La exclusi√≥n de predictores bas√°ndose en p-values no es aconsejable, en su lugar se recomienda emplear m√©todos de best subset selection, stepwise selection (forward, backward e hybrid) o Shrinkage/regularization.

```{r}
step(modelo_multiple, direction = "both", trace = 0)
```

La selecci√≥n de predictores empleando stepwise selection (hybrid/doble) ha identificado como mejor modelo el formado por los predictores crim, zn, chas, nox, rm, dis, rad, tax, ptratio, black, lstat.

```{r}
modelo_multiple <- lm(formula = medv ~ crim + zn + chas +  nox + rm +  dis +
                      rad + tax + ptratio + black + lstat, data = Boston)
# Tambi√©n se pueden indicar todas las variables de un data.frame y exluir algunas
# modelo_multiple <- lm(formula = medv~. -age -indus, data = Boston)
summary(modelo_multiple)
```

### Supuestos 

En los modelos de regresi√≥n lineal con m√∫ltiples predictores, adem√°s del estudio de los residuos vistos en el modelo simple, es necesario descartar colinealidad o multicolinealidad entre variables.


```{r}
par(mfrow = c(1,2))
plot(modelo_multiple)
par(mfrow = c(1,1))
```

Para la colinealidad se recomienda calcular el coeficiente de correlaci√≥n entre cada par de predictores incluidos en el modelo:

```{r}
require(corrplot)
corrplot.mixed(corr = cor(Boston[,c("crim", "zn", "nox", "rm", "dis", "rad", 
                                    "tax", "ptratio", "black", "lstat", "medv")],
                          method = "pearson"))
```

El an√°lisis muestra correlaciones muy altas entre los predictores rad y tax (positiva) y entre dis y nox (negativa).

```{r}
attach(Boston)
par(mfrow = c(2,2))
plot(x = tax, y = rad, pch = 20)
plot(x = tax, y = nox, pch = 20)
plot(x = dis, y = nox, pch = 20)
plot(x = medv, y = rm, pch = 20)
par(mfrow = c(1,1))
```

Si la correlaci√≥n es alta y por lo tanto las variables aportan informaci√≥n redundante, es recomendable analizar si el modelo mejora o no empeora excluyendo alguno de estos predictores.

Para el estudio de la multicolinealidad una de las medidas m√°s utilizadas es el factor de inflaci√≥n de varianza VIF. Puede calcularse mediante la funci√≥n vif() del paquete car.

```{r}
require(car)
vif(modelo_multiple)
```

Los indices VIF son bajos o moderados, valores entre 5 y 10 indican posibles problemas y valores mayores o iguales a 10 se consideran muy problem√°ticos.

De igual forma, podemos realizar la predicci√≥n para el modelo m√∫ltiple pero indicando el valor a predir para cada variable.

## Regresi√≥n Polinomial: incorporar no-linealidad a los modelos lineales

La Regresi√≥n Polinomial, aunque permite describir relaciones no lineales, se trata de un modelo lineal en el que se incorporan nuevos predictores elevando el valor de los ya existentes a diferentes potencias.

Cuando se intenta predecir el valor de la vivienda en funci√≥n del estatus de la poblaci√≥n, el modelo lineal generado no se ajusta del todo bien debido a que las observaciones muestran una relaci√≥n entre ambas variables con cierta curvatura.

```{r}
attach(Boston)
plot(x = lstat, y = medv, main = "medv vs lstat", pch = 20, col = "grey30")
abline(modelo_simple, lwd = 3, col = "red")
```

La curvatura descrita apunta a una posible relaci√≥n cuadr√°tica, por lo que un polinomio de segundo grado podr√≠a capturar mejor la relaci√≥n entre las variables. En R se pueden generar modelos de regresi√≥n polin√≥mica de diferentes formas:

Identificando cada elemento del polinomio: modelo_pol2 <- lm(formula = medv ~ lstat + I(lstat^2), data = Boston) El uso de I() es necesario ya que el s√≠mbolo ^ tiene otra funci√≥n dentro de las formula de R.

Con la funci√≥n poly(): lm(formula = medv ~ poly(lstat, 2), data = Boston)

```{r}
modelo_pol2 <- lm(formula = medv ~ poly(lstat, 2), data = Boston)
summary(modelo_pol2)
```

El p-value pr√≥ximo a 0 del predictor cuadr√°tico de lstat indica que contribuye a mejorar el modelo.

```{r}
attach(Boston)
plot(x = lstat, y = medv, main = "medv vs lstat", pch = 20, col = "grey30")
points(lstat, fitted(modelo_pol2), col = 'red', pch = 20)
```

A la hora de comparar dos modelos se pueden evaluar sus R2. En este caso el modelo cuadr√°tico es capaz de explicar un 64% de variabilidad frente al 54% del modelo lineal.

Dado que un polinomio de orden n siempre va a estar anidado a uno de orden n+1, se pueden comparar modelos polin√≥micos dentro un rango de grados haciendo comparaciones secuenciales.

```{r}
anova(modelo_simple, modelo_pol2)
```

El p-value obtenido para el estad√≠stico F confirma que el modelo cuadr√°tico es superior.


# 8. Regresi√≥n dicot√≥mica.

Si en vez de una **variable continua** tenemos una **variable categ√≥rica**, y esa desea ser explicada por un conjunto de predictores o variables independienntes, podemos aplicar una regresi√≥n dicot√≥mica. 

Existen dos grandes tipos:

- Log√≠stica
- Probit

## Datos

- **Descripci√≥n:** registros con informaci√≥n de 79 muestras de orina con caracter√≠sticas f√≠sicas.
- **Variables:** est√°n registradas las siguientes variables:
    - **`r`**: indicador de presencia de oxalato de calcio. <tred>(Variable Objetivo)</tred>
        - **`0`:** ausencia de oxalato en orina.
        - **`1`:** presencia de oxalato en orina.
    - **`gravity`**: gravedad espec√≠fica de la orina.
    - **`ph`**: pH de la orina.
    - **`osmo`**: osmolaridad de la orina.
    - **`cond`**: conductivicad de la orina.
    - **`urea`**: concentraci√≥n de urea en la orina.
    - **`calc`**: concentraci√≥n de calcio (milimoles por litro).
- **Problema:** determinar si una o m√°s caracter√≠sticas f√≠sicas de la orina est√°n relacionadas con la presencia de oxalatos. Adem√°s, generar un modelo capaz de clasificar pacientes con presencia de cristales que podr√≠an ser causantes de patolog√≠as (c√°lculos renales).

```{r}
datos <- read.csv("C:/Users/oscar/Desktop/R --- SAF/Tema 7/Orina.csv")

# Conversi√≥n de variable objetivo a factor
datos$r <- as.factor(datos$r)

# Imprimiendo datos
tail(datos)
```

## La regresi√≥n log√≠stica

- La regresi√≥n log√≠stica analiza datos con [*distribuci√≥n binomial*](https://en.wikipedia.org/wiki/Binomial_distribution) de la forma:

$$Y_i \sim\ B(p_i,\ n_i),\ para\ i=1,...,m$$

- En la expresi√≥n anterior $p_i$ hace referencia a la probabilidad de √©xito (probabilidad de que ocurra el evento bajo estudio) y $n_i$ determina el n√∫mero de ensayos tipo *Bernoulli*. El n√∫mero de ensayos es conocido, sin embargo, la probabilidad del √©xito se desconoce.
- Se debe cumplir que la respuesta est√© acotada entre 0 y 1, es decir, que el resultado siempre ser√° positivo, adem√°s de ser inferior a 1.
- El exponencial ($e$) de cualquier valor ($x$) es siempre positivo y, cualquier n√∫mero divivido entre la cantidad m√°s uno ($x+1$) siempre ser√° menor que 1. Bajo estas dos premisas se puede expresar la siguiente probabilidad condicional (funci√≥n log√≠stica):

$$p(Y =1\ |\ X)=\frac{e^{(\beta_0+\beta_1x)}}{e^{(\beta_0+\beta_1x)}+1}$$

- Para facilitar el c√°lculo escribimos $p(Y =1\ |\ X)$ como $p(X)$:

$$p(X) = \frac{e^{(\beta_0+\beta_1x)}}{e^{(\beta_0+\beta_1x)}+1}\\
p(e^{(\beta_0+\beta_1x)}+1) = e^{(\beta_0+\beta_1x)}\\
p \times e^{(\beta_0+\beta_1x)}\ +\ p = e^{(\beta_0+\beta_1x)}\\
p = e^{(\beta_0+\beta_1x)}\ -\ p \times e^{(\beta_0+\beta_1x)}\\
p = e^{(\beta_0+\beta_1x)}(1-p)\\
\frac{p}{1-p} = e^{(\beta_0+\beta_1x)}$$

- Los *logits* (funci√≥n de enlace) de las probabilidades binomiales desconocidas, es decir, los logaritmos de la [*raz√≥n de momios (odds ratio)*](https://es.wikipedia.org/wiki/Raz%C3%B3n_de_momios) son modelados como una funci√≥n lineal de los $X_i$:

$$ln(\frac{p}{1-p}) = \beta_0+\beta_1x$$

- Esta funci√≥n de enlace es conocida como *sigmoide* y limita su rango de probabilidades entre 0 y 1.


## La regresi√≥n probit

- La regresi√≥n probit permite analizar datos con respuesta ordinal o con *distribuci√≥n binomial* (respuestas dicot√≥micas) de la forma:

$$Y_i \sim\ B(p_i,\ n_i),\ para\ i=1,...,m$$

- El marco conceptual del modelo probit puede ser expresado de la siguiente manera:

$$p(Y = 1|X)=\ \Phi(X^{T}\beta)$$

- Donde $p(Y = 1|X)$ denota la probabilidad, $\Phi$ es la funci√≥n de distribuci√≥n acumulativa de la distribuci√≥n normal est√°ndar y $\beta$ son los par√°metros del modelo, estimados a trav√©s de m√°xima verosimilitud.
- El modelo puede ser expresado de la siguiente manera: $Y = X^{T}\beta+\epsilon$, donde $\epsilon \sim N(0, 1)$.
- Las funciones log√≠stica y probit difieren en la manera como definen la funci√≥n de distribuci√≥n, mientras que la primera utiliza la funci√≥n log√≠stica la segunda hace uso de la funci√≥n de distribuci√≥n acumulada de la normal est√°ndar. Ambas funciones pueden ser comparadas en la siguiente figura:

### Estad√≠sticos descripvitos

```{r}
library(tidyr)
datos %>% 
  gather(key = "variable", value = "valor", -r) %>% 
  group_by(variable, r) %>% 
  summarise(Promedio = round(mean(valor, na.rm = TRUE), digits = 2),
            `D. Est√°ndar` = round(sd(valor, na.rm = TRUE), digits = 2),
            M√≠nimo = round(min(valor, na.rm = TRUE), digits = 2),
            M√°ximo = round(max(valor, na.rm = TRUE), digits = 2),
            Q1 = round(quantile(valor, prob = 0.25, na.rm = TRUE), digits = 2),
            Q2 = round(quantile(valor, prob = 0.5, na.rm = TRUE), digits = 2),
            Q3 = round(quantile(valor, prob = 0.75, na.rm = TRUE), digits = 2)) %>% 
  rename(Oxalato = r, Variable = variable) %>% 
  datatable()
```

An√°lisis Exploratorio

### Datos ausentes

```{r, echo=TRUE}
library(broom)
tidy(apply(datos, MARGIN = 2, is.na)) %>% 
  gather(key = "variable", value = "valor") %>% 
  mutate(valor = as.numeric(valor))  %>% 
  group_by(variable) %>% 
  summarise(Total_NAs = sum(valor))
```

### Densidades

```{r, fig.height=5}
library(ggplot2)
colores <- c("dodgerblue", "gray40")
datos %>% 
  rename(Oxalato = r) %>% 
  gather(key = "variable", value = "valor", -Oxalato) %>% 
  ggplot(data = ., aes(x = valor, fill = Oxalato)) +
  facet_wrap(~variable, scales = "free") +
  geom_density(alpha = 0.7) +
  scale_fill_manual(values = colores) +
  labs(x = "", y = "") +
  theme_light() +
  theme(legend.position = "bottom")
```

### Distribuci√≥n condicional

- Las densidades condicionales son gr√°ficos exploratorios que permiten dilucidar c√≥mo es la probabilidad de "√©xito" o "fracaso" respecto a variables num√©ricas. Es posible evidenciar en qu√© puntos (valores de x) se maximiza la probabilidad de "√©xito", que en este caso est√° ligado a la presencia (<tred>r = 1</tred>) de oxalatos en orina.

```{r}
par(mfrow = c(2, 3))
cdplot(datos$r ~ datos$gravity, xlab = "Gravedad espec√≠fica",
       ylab = "Oxalato", col = colores)
cdplot(datos$r ~ datos$ph, xlab = "pH",
       ylab = "Oxalato", col = colores)
cdplot(datos$r ~ datos$osmo, xlab = "Osmolaridad",
       ylab = "Oxalato", col = colores)
cdplot(datos$r ~ datos$cond, xlab = "Conductividad",
       ylab = "Oxalato", col = colores)
cdplot(datos$r ~ datos$urea, xlab = "Urea",
       ylab = "Oxalato", col = colores)
cdplot(datos$r ~ datos$calc, xlab = "Calcio",
       ylab = "Oxalato", col = colores)
```

### Boxplot comparativo

```{r, fig.height=5}
datos %>% 
  rename(Oxalato = r) %>% 
  gather(key = "variable", value = "valor", -Oxalato) %>% 
  ggplot(data = ., aes(x = Oxalato, y = valor, fill = Oxalato)) +
  facet_wrap(~variable, scales = "free") +
  geom_boxplot() +
  scale_fill_manual(values = colores) +
  labs(x = "", y = "") +
  theme_light() +
  theme(legend.position = "bottom")
```


## Estimaci√≥n de los modelos


```{r, echo=TRUE}
# Modelos Lineales Generalizados
mod_logit  <- glm(r ~ ., data = datos, family = binomial(link = "logit"))
mod_probit <- glm(r ~ ., data = datos, family = binomial(link = "probit"))

# -------- Validaci√≥n LOOCV (Manual)
out_logi_0.5 <- NULL
out_prob_0.5 <- NULL
for(i in 1:nrow(datos)){
  out_logi_0.5[i] = predict(update(mod_logit, data = datos[-i, ]),
                   newdata = datos[i,], type = "response")
  out_prob_0.5[i] = predict(update(mod_probit, data = datos[-i, ]),
                   newdata = datos[i,], type = "response")
}

# -------- Validaci√≥n LOOCV (cv.glm())

## Funci√≥n de coste con cutoff = 0.5
coste_0.5 <- function(r, pi = 0) mean(abs(r-pi)> 0.5)

## LOOCV
library(boot)
cv_error_logi_0.5 <- cv.glm(data = datos %>% filter(!is.na(osmo) & !is.na(cond)),
                        glmfit = mod_logit, cost = coste_0.5)
cv_error_prob_0.5 <- cv.glm(data = datos %>% filter(!is.na(osmo) & !is.na(cond)),
                        glmfit = mod_probit, cost = coste_0.5)
```


- <tred>**Observaciones:**</tred> la ejecuci√≥n autom√°tica de *LOOCV* con la funci√≥n `cv.glm()` requiere una funci√≥n de coste para calcular el error. El objeto devuelto por la funci√≥n del paquete `boot` almacena el error con el nombre `delta`; al restar 1 menos el error (delta) se obtendr√° la precisi√≥n o *accuracy* del modelo, que es exactamente el mismo valor obtenido manualmente.
  
- **Funci√≥n de coste:** en el c√≥digo hay una funci√≥n de coste o p√©rdida para el l√≠mite igual a 0.5. Esta funci√≥n puede ser expresada de la siguiente manera: $error = \sum |r_i - p_i| > 0.5$. Donde $r_i$ es el i-√©simo valor real y $p_i$ es el i-√©simo valor predicho. En el siguiente c√≥digo es posible evidenciar que se obtienen los mismos resultados de forma manual y con la funci√≥n `cv.glm()`.

## Finalidad 

Los modelos dicot√≥micos tambi√©n sirven para la predicci√≥n, y seg√∫n cierta condici√≥n, brindar√°n una probabilidad (entre 0-1) sobre un evento que as√≠ se consulte a trav√©s del modelo estimado.



# 9. Series temporales.

El an√°lisis de las series temporales juega un rol esencial tanto para conocer un determinado fen√≥meno como para pronosticar hacia el futuro.

Las series de tiempo son observaciones sobre un determinado fen√≥meno efectuadas en el tiempo, en lapsos ojala equivalente, o con intervalos de igual valor.

Ejemplos: exportaciones mensuales, ventas diarias de un producto, casos semanales de sida, temperatura promedio diaria, tasa anual de mortalidad, numero mensual de divorcios.

Una serie de tiempo posee dos caracter√≠sticas esenciales:

Los valores est√°n ordenados o presentados en el tiempo.

Existe una dependencia o correlaci√≥n entre los valores de la serie en el tiempo.

Un an√°lisis por series temporales podr√≠a buscar:

Descripci√≥n de la serie
Adecuar un modelo o t√©cnica estoc√°stica
Pronostico en un periodo h de la serie.

El an√°lisis de la serie debe preguntarse sobre el tipo de serie que se est√° analizando, los tipos de datos, y el per√≠odo de referencia en la adecuaci√≥n del mejor modelo estoc√°stico.

De igual forma, dependiendo de la serie, el pron√≥stico debe considerar ciertas restricciones.

El an√°lisis de una serie de tiempo de proceder, sin ser restrictivo, procede a groso modo de la siguiente forma:

- Descripci√≥n de la serie temporal.
- Selecci√≥n de datos: rango de modelizaci√≥n, rango de estimaci√≥n, pron√≥stico.
- Estimaci√≥n de los modelos.
- Medidas de rendimiento (set de entrenamiento).
- Estimaci√≥n al set de validaci√≥n.
- Medidas de rendimiento (set de validaci√≥n).
- Selecci√≥n del mejor m√©todo de estimaci√≥n
- Pron√≥stico.

## Predicci√≥n vs. Pron√≥stico

Para faciliar las cosas, hablemos de la regresi√≥n en un contexto transversal y otro longitudinal.

![](transversal-longitudinal.png)

En un contexto transversal, nuestro puntos o sujetos de estudio son las personas en un momento *ti*.
En un contexto longitudinal, nuestro puntos o sujetos de estudio son los peridos de todo un rango de per√≠odos *t*.

Mientras que la predicci√≥n busca saber el efecto de los coeficientes y realizar una intrapolaci√≥n,
en el pron√≥stico buscamos lo opuesto: extrapolamos y no le damos importancia al valor de los coeficientes.

![](prediccion-pronostico.png)

## Ejemplo  Pron√≥stico del nivel del CO2  

Primero cargamos los datos y los transformados en ts (time series).


```{r}
co2 <- read.csv("C:/Users/oscar/Desktop/R --- SAF/Tema 7/co2.csv", sep=";")

co2ts<-ts(co2$CO2, start = c(1959,1), frequency = 12)
```

La estructura de una serie ts:

```{r}
print(co2ts)
```

### Visualizaci√≥n de la serie

Veamos la serie como tal:

```{r}
autoplot(co2ts, ts.colour = "blue", ts.linetype = "dashed")
```

¬øQu√© podemos decir? ¬øHay autocorrelaci√≥n y estacionalidad en el tiempo?

Un gr√°fico de los autocorrelaciones totales nos puede ayudar:

```{r}
autoplot(acf(co2ts, plot = FALSE))
```

Vemos un estudio de los componentes de una serie:

```{r}
autoplot(stl(co2ts, s.window = "periodic"), ts.colour = "blue")
```

### Estimaci√≥n de modelos de series de tiempo: ARIMA

Vamos a estimar un total de 6 modelos ARIMA y luego comprar al mejor entre ellos:

- ARIMA(0,1,2)(0,1,1)
- ARIMA(1,1,0)(2,1,0)
- ARIMA(1,1,2)(2,1,1)
- ARIMA(1,1,1)(2,1,1)
- ARIMA(1,1,2)(1,1,1)
- ARIMA(0,1,1)(0,1,1)
- ARIMA(1,1,0)(1,1,0)

```{r}
arima1<- Arima(co2ts, order=c(0,1,2), seasonal=list(order=c(0,1,1),period=12))
arima2<- Arima(co2ts, order=c(1,1,0), seasonal=list(order=c(2,1,0),period=12))
arima3<- Arima(co2ts, order=c(1,1,2), seasonal=list(order=c(2,1,1),period=12))
arima4<- Arima(co2ts, order=c(1,1,1), seasonal=list(order=c(2,1,1),period=12))
arima5<- Arima(co2ts, order=c(1,1,2), seasonal=list(order=c(1,1,1),period=12))
arima6<- Arima(co2ts, order=c(0,1,1), seasonal=list(order=c(0,1,1),period=12))
arima7<- Arima(co2ts, order=c(1,1,0), seasonal=list(order=c(1,1,0),period=12))
```

Veamos las salidas del **ARIMA(0,1,2)(0,1,1)**

```{r}
arima1
```

¬øQu√© nos dice la salida? ¬øC√≥mo interpretamos los coeficientes?

Veamos cu√°l es el mejor modelo entre los 6 estimados. Utilizaremos el criterio de AIC y BIC:

```{r}
AIC(arima1,arima2,arima3,arima4,arima5,arima6,arima7)
BIC(arima1,arima2,arima3,arima4,arima5,arima6,arima7)
```

Aqu√≠ se puede apreciar que los ajustes que mejor AIC y BIC presentan son aquellas que solo tienen componente de m√©dias m√≥viles y no tienen componente autorregresiva, siendo ARIMA(0,1,1)(0,1,1) el modelo que los test arrojan con un menor valor y por tanto con una mayor consideraci√≥n.

Una vez estimados los modelos y elegido el mejor de ellos, en este caso ARIMA(0,1,1)(0,1,1), se procede a validarlo, esto es, ver la adecuidad del modelo seleccionado. Para ello en primer lugar se grafican los correlogramas de los residuos para comprobar que son ruido blanco:

```{r}
ggtsdiag(arima6)
```

El modelo respecta los criterior de adecuaci√≥n.

### Pron√≥stico --> horizonte de proyecci√≥n h

Finalizada la etapa de estimaci√≥n, selecci√≥n y validaci√≥n de un modelo candidato, vamos a proyectar para los siguientes cuatro a√±os siguientes, lo cu√°l son 48 meses, o un horizonte de proyecci√≥n h=48.

```{r}
forecast1<-forecast(arima6, level = c(95), h = 50)
autoplot(forecast1)
```


## Otras t√©cnicas de estimaci√≥n.

En ete caso solo se detall√≥ una estimaci√≥n por un modelo ARIMA, pero para series univariadas se podr√≠a optar por:

- Regresi√≥n en el tiempo
- Descomposici√≥n de una TS
- ETS
- Regresiones din√°micas 
- Redes neuronales
- Entre otros...

# Fin

![](the-end.jpg)
